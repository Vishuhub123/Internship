{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91644aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing selenium\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings .filterwarnings(\"ignore\")\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb420ae",
   "metadata": {},
   "source": [
    "## Q1\n",
    ". Write a python program which searches all the product under a particular product from www.amazon.in.  The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars.\n",
    " \n",
    "## Q2.\n",
    "In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "589f9bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to web driver\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "# or driver=webdriver.Chrome(r'\"C:\\Users\\hp\\Dropbox\\PC\\Documents\\chromedriver.exe\"')\n",
    "url=' https://www.amazon.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cd177a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guitar\n"
     ]
    }
   ],
   "source": [
    "# finding web element for search using id\n",
    "Product=driver.find_element_by_id('twotabsearchtextbox')\n",
    "Product\n",
    "# writing on search bar\n",
    "Product.send_keys(input())\n",
    "\n",
    "# click button using xpath\n",
    "search_btn=driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65f83215",
   "metadata": {},
   "source": [
    " Now for part 2 where we have to extract \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \n",
    " \"Expected Delivery\", \"Availability\" and “Product URL”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9519f7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#for extracting data from multiple pages weare using for loop along with exception handling\n",
    "start=0\n",
    "end=3\n",
    "\n",
    "URL=[]\n",
    "for page in range(start,end):\n",
    "    try:\n",
    "        for i in driver.find_elements_by_xpath('//a[@class=\"a-link-normal s-link-style a-text-normal\"]'):   \n",
    "            URL.append(i.get_attribute(\"href\"))                      # gathering url from href attribute\n",
    "    except:\n",
    "        URL.append(\"-\")\n",
    "# for going to next page\n",
    "    time.sleep(2)\n",
    "    nxt_page=driver.find_elements_by_xpath('//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]')\n",
    "    try:\n",
    "        driver.get(nxt_page[0].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(nxt_page[1].get_attribute('href'))\n",
    "        \n",
    "\n",
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7124acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "brand = []\n",
    "name = []\n",
    "price = []\n",
    "exchange = []\n",
    "delivery=[]\n",
    "availability=[]\n",
    "for i in URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    try:                        \n",
    "        brnd=driver.find_element_by_xpath('//td[@class=\"a-span9\"]/span')\n",
    "        brand.append(brnd.text)                                  \n",
    "    except NoSuchElementException:\n",
    "        brand.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        nm=driver.find_element_by_xpath('//h1[@class=\"a-size-large a-spacing-none\"]/span')            ## product name\n",
    "        name.append(nm.text)\n",
    "    except NoSuchElementException:\n",
    "        price.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        prc=driver.find_element_by_xpath('//div[@class=\"a-section\"]/span')      # price\n",
    "        price.append(prc.text)\n",
    "    except NoSuchElementException:\n",
    "        price.append(\"-\")\n",
    "         \n",
    "    try:\n",
    "        exc=driver.find_element_by_xpath('//div[@class=\"a-section a-spacing-none icon-content\"]/a')            # exchange\n",
    "        exchange.append(exc.text)\n",
    "    except NoSuchElementException:\n",
    "        exchange.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        de=driver.find_element_by_xpath('//div[@class=\"a-section a-spacing-mini\"]/b')\n",
    "        delivery.append(de.text)\n",
    "    except NoSuchElementException:\n",
    "        delivery.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        ava=driver.find_element_by_xpath('//span[@class=\"a-size-medium a-color-success\"]')\n",
    "        availability.append(ava.text)\n",
    "    except NoSuchElementException:\n",
    "        availability.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6538a63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "184\n",
      "184\n",
      "184\n",
      "184\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "print(len(brand))              #here I have checked the length of each data so that while creating a dataframe I don't face any error.\n",
    "print(len(price)) \n",
    "print(len(name))\n",
    "print(len(exchange))\n",
    "print(len(delivery))\n",
    "print(len(availability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "91e25c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Name of the Product</th>\n",
       "      <th>Price</th>\n",
       "      <th>Return/Exchange</th>\n",
       "      <th>Expected Delivery</th>\n",
       "      <th>Availability</th>\n",
       "      <th>Product URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kadence</td>\n",
       "      <td>Kadence Frontier Jumbo Semi Acoustic Guitar Wi...</td>\n",
       "      <td></td>\n",
       "      <td>Pay on Delivery</td>\n",
       "      <td>Sunday, Jan 30</td>\n",
       "      <td></td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VAULT</td>\n",
       "      <td>Vault EA40 Premium 41 inch Spruce-Top Cutaway ...</td>\n",
       "      <td>₹6,990\\n00</td>\n",
       "      <td>Pay on Delivery</td>\n",
       "      <td>Friday, Jan 28</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kadence</td>\n",
       "      <td>Kadence Guitar Acoustica Series, Electric Acou...</td>\n",
       "      <td></td>\n",
       "      <td>Pay on Delivery</td>\n",
       "      <td>Sunday, Jan 30</td>\n",
       "      <td></td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kadence</td>\n",
       "      <td>Kadence Frontier guitar with Online Guitar lea...</td>\n",
       "      <td></td>\n",
       "      <td>Pay on Delivery</td>\n",
       "      <td>Friday, Jan 28</td>\n",
       "      <td></td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JUAREZ</td>\n",
       "      <td>Juarez Acoustic Guitar, 38 Inch Curved Body Cu...</td>\n",
       "      <td>₹2,190\\n00</td>\n",
       "      <td>Pay on Delivery</td>\n",
       "      <td>Friday, Jan 28</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>https://www.amazon.in/Juarez-Acoustic-Guitar-C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>YAMAHA</td>\n",
       "      <td>Yamaha FS100C Acoustic Guitar Concert body Wit...</td>\n",
       "      <td>₹12,831\\n00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Sunday, Jan 30</td>\n",
       "      <td>Only 1 left in stock.</td>\n",
       "      <td>https://www.amazon.in/Yamaha-FS100C-6-Strings-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>MUSTANG</td>\n",
       "      <td>Pack of 2 Universal Guitar Stand by Hola! Musi...</td>\n",
       "      <td>₹999\\n00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Monday, Jan 31</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Techblaze</td>\n",
       "      <td>Techblaze 6 Pieces Guitar Keys for Acoustic Gu...</td>\n",
       "      <td>₹549\\n00</td>\n",
       "      <td>Pay on Delivery</td>\n",
       "      <td>Monday, Jan 31</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>TECHBLAZE</td>\n",
       "      <td>TECHBLAZE 6 Pcs Guitar Keys for Acoustic Guita...</td>\n",
       "      <td>₹589\\n00</td>\n",
       "      <td>Pay on Delivery</td>\n",
       "      <td>Monday, Jan 31</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>HRB MUSICALS</td>\n",
       "      <td>HRB MUSICALS combo guitar stand &amp; guitar bag N...</td>\n",
       "      <td>₹589\\n00</td>\n",
       "      <td>7 Days Replacement</td>\n",
       "      <td>Monday, Jan 31</td>\n",
       "      <td>In stock.</td>\n",
       "      <td>https://www.amazon.in/gp/slredirect/picassoRed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Brand Name                                Name of the Product  \\\n",
       "0         Kadence  Kadence Frontier Jumbo Semi Acoustic Guitar Wi...   \n",
       "1           VAULT  Vault EA40 Premium 41 inch Spruce-Top Cutaway ...   \n",
       "2         Kadence  Kadence Guitar Acoustica Series, Electric Acou...   \n",
       "3         Kadence  Kadence Frontier guitar with Online Guitar lea...   \n",
       "4          JUAREZ  Juarez Acoustic Guitar, 38 Inch Curved Body Cu...   \n",
       "..            ...                                                ...   \n",
       "179        YAMAHA  Yamaha FS100C Acoustic Guitar Concert body Wit...   \n",
       "180       MUSTANG  Pack of 2 Universal Guitar Stand by Hola! Musi...   \n",
       "181     Techblaze  Techblaze 6 Pieces Guitar Keys for Acoustic Gu...   \n",
       "182     TECHBLAZE  TECHBLAZE 6 Pcs Guitar Keys for Acoustic Guita...   \n",
       "183  HRB MUSICALS  HRB MUSICALS combo guitar stand & guitar bag N...   \n",
       "\n",
       "           Price     Return/Exchange Expected Delivery           Availability  \\\n",
       "0                    Pay on Delivery    Sunday, Jan 30                          \n",
       "1     ₹6,990\\n00     Pay on Delivery    Friday, Jan 28              In stock.   \n",
       "2                    Pay on Delivery    Sunday, Jan 30                          \n",
       "3                    Pay on Delivery    Friday, Jan 28                          \n",
       "4     ₹2,190\\n00     Pay on Delivery    Friday, Jan 28              In stock.   \n",
       "..           ...                 ...               ...                    ...   \n",
       "179  ₹12,831\\n00  7 Days Replacement    Sunday, Jan 30  Only 1 left in stock.   \n",
       "180     ₹999\\n00  7 Days Replacement    Monday, Jan 31              In stock.   \n",
       "181     ₹549\\n00     Pay on Delivery    Monday, Jan 31              In stock.   \n",
       "182     ₹589\\n00     Pay on Delivery    Monday, Jan 31              In stock.   \n",
       "183     ₹589\\n00  7 Days Replacement    Monday, Jan 31              In stock.   \n",
       "\n",
       "                                           Product URL  \n",
       "0    https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "1    https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "2    https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "3    https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "4    https://www.amazon.in/Juarez-Acoustic-Guitar-C...  \n",
       "..                                                 ...  \n",
       "179  https://www.amazon.in/Yamaha-FS100C-6-Strings-...  \n",
       "180  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "181  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "182  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "183  https://www.amazon.in/gp/slredirect/picassoRed...  \n",
       "\n",
       "[184 rows x 7 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the DataFrame - \n",
    "Guitar = pd.DataFrame({'Brand Name':brand,'Name of the Product':name,'Price':price,'Return/Exchange':exchange,'Expected Delivery':delivery,'Availability':availability,'Product URL':URL})\n",
    "Guitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c451730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame as csv file for our future use.\n",
    "Guitar.to_csv(\"Guitar Details.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59bf38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88bf00a8",
   "metadata": {},
   "source": [
    "## Q3.\n",
    "Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0688cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to web driver\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "url='https://www.google.com/imghp?hl=en'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2614588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fruits\n"
     ]
    }
   ],
   "source": [
    "# finding web element for search using id\n",
    "ig=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "ig\n",
    "# writing on search bar\n",
    "ig.send_keys(input())\n",
    "time.sleep(3)\n",
    "# click button using xpath\n",
    "search_btn=driver.find_element_by_xpath(\"//span[@class='z1asCe MZy1Rb']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "603537a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")                       #Scrolling the windows of the search page.\n",
    "    image = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "img_urls = []                                                               #Empty list\n",
    "for img in image: \n",
    "    source = img.get_attribute('src')                                    #Source of the image's\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f53d69ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 10 images\n",
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "import requests                                                    \n",
    "for i in range(len(img_urls)):                                             #Here we have used range fuc. and inside that we have given len fuc.(due to this our code will run the same no. of time as the lenth of img.)\n",
    "    if i>10:                                                               #We want to download 10 images.\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\image\\q3\"+str(i)+\".jpg\",\"wb\")   #Opeing the folder location where we want to download\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3bdd493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cars\n",
    "#connecting to web driver\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "url='https://www.google.com/imghp?hl=en'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de32ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars\n"
     ]
    }
   ],
   "source": [
    "# finding web element for search using id\n",
    "ig=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "ig\n",
    "# writing on search bar\n",
    "ig.send_keys(input())\n",
    "time.sleep(3)\n",
    "# click button using xpath\n",
    "search_btn=driver.find_element_by_xpath(\"//span[@class='z1asCe MZy1Rb']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ad4ee308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 10 images\n",
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")                       #Scrolling the windows of the search page.\n",
    "    image = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "img_urls = []                                                               #Empty list\n",
    "for img in image: \n",
    "    source = img.get_attribute('src')                                    #Source of the image's\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls) \n",
    "for i in range(len(img_urls)):                                             #Here we have used range fuc. and inside that we have given len fuc.(due to this our code will run the same no. of time as the lenth of img.)\n",
    "    if i>10:                                                               #We want to download 10 images.\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\image\\q3\"+str(i)+\".jpg\",\"wb\")   #Opeing the folder location where we want to download\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9b43ee17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning\n"
     ]
    }
   ],
   "source": [
    "# for machine learning\n",
    "#connecting to web driver\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "url='https://www.google.com/imghp?hl=en'\n",
    "driver.get(url)\n",
    "# finding web element for search using id\n",
    "ig=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "ig\n",
    "# writing on search bar\n",
    "ig.send_keys(input())\n",
    "time.sleep(3)\n",
    "# click button using xpath\n",
    "search_btn=driver.find_element_by_xpath(\"//span[@class='z1asCe MZy1Rb']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e6d493c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 10 images\n",
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")                       #Scrolling the windows of the search page.\n",
    "    image = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "img_urls = []                                                               #Empty list\n",
    "for img in image: \n",
    "    source = img.get_attribute('src')                                    #Source of the image's\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls) \n",
    "for i in range(len(img_urls)):                                             #Here we have used range fuc. and inside that we have given len fuc.(due to this our code will run the same no. of time as the lenth of img.)\n",
    "    if i>10:                                                               #We want to download 10 images.\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\image\\q3\"+str(i)+\".jpg\",\"wb\")   #Opeing the folder location where we want to download\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2a79ad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guitar\n"
     ]
    }
   ],
   "source": [
    "# for guitar\n",
    "#connecting to web driver\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "url='https://www.google.com/imghp?hl=en'\n",
    "driver.get(url)\n",
    "# finding web element for search using id\n",
    "ig=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "ig\n",
    "# writing on search bar\n",
    "ig.send_keys(input())\n",
    "time.sleep(3)\n",
    "# click button using xpath\n",
    "search_btn=driver.find_element_by_xpath(\"//span[@class='z1asCe MZy1Rb']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e0e80fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 10 images\n",
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")                       #Scrolling the windows of the search page.\n",
    "    image = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "img_urls = []                                                               #Empty list\n",
    "for img in image: \n",
    "    source = img.get_attribute('src')                                    #Source of the image's\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls) \n",
    "for i in range(len(img_urls)):                                             #Here we have used range fuc. and inside that we have given len fuc.(due to this our code will run the same no. of time as the lenth of img.)\n",
    "    if i>10:                                                               #We want to download 10 images.\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\image\\q3\"+str(i)+\".jpg\",\"wb\")   #Opeing the folder location where we want to download\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ae577cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cakes\n"
     ]
    }
   ],
   "source": [
    "# for cakes\n",
    "#connecting to web driver\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "url='https://www.google.com/imghp?hl=en'\n",
    "driver.get(url)\n",
    "# finding web element for search using id\n",
    "ig=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "ig\n",
    "# writing on search bar\n",
    "ig.send_keys(input())\n",
    "time.sleep(3)\n",
    "# click button using xpath\n",
    "search_btn=driver.find_element_by_xpath(\"//span[@class='z1asCe MZy1Rb']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8ccc14fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 10 images\n",
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")                       #Scrolling the windows of the search page.\n",
    "    image = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "img_urls = []                                                               #Empty list\n",
    "for img in image: \n",
    "    source = img.get_attribute('src')                                    #Source of the image's\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls) \n",
    "for i in range(len(img_urls)):                                             #Here we have used range fuc. and inside that we have given len fuc.(due to this our code will run the same no. of time as the lenth of img.)\n",
    "    if i>10:                                                               #We want to download 10 images.\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\image\\q3\"+str(i)+\".jpg\",\"wb\")   #Opeing the folder location where we want to download\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ee368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d34194ca",
   "metadata": {},
   "source": [
    "## Q4.\n",
    "Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on\n",
    "www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be \n",
    "scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, \n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the \n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78aebafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to web driver\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90ed5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding web element for search using id\n",
    "smartphone=driver.find_element_by_class_name(\"_3704LK\")\n",
    "smartphone\n",
    "# writing on search bar\n",
    "smartphone.send_keys(\"realme\")\n",
    "\n",
    "# click button using xpath\n",
    "search_btn=driver.find_element_by_class_name(\"L0Z3Pu\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29c25583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#for extracting data from multiple pages weare using for loop along with exception handling\n",
    "\n",
    "url=[]\n",
    "try:\n",
    "    for i in driver.find_elements_by_xpath('//div[@class=\"_2kHMtA\"]/a'): \n",
    "        url.append(i.get_attribute(\"href\"))\n",
    "           \n",
    "except:\n",
    "     url.append(\"-\")\n",
    "        \n",
    "\n",
    "len(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5de06cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "brand = []\n",
    "name = []\n",
    "colour = []\n",
    "ram = []\n",
    "rom=[]\n",
    "pcam=[]\n",
    "scam=[]\n",
    "dsize=[]\n",
    "battery=[]\n",
    "price=[]\n",
    "for i in url:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    but=driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]')\n",
    "    but.click()\n",
    "    try:                        \n",
    "        b=driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')      # for brand\n",
    "        brand.append(b.text.split()[0]) \n",
    "    except NoSuchElementException:\n",
    "        brand.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        n=driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')            ## product name\n",
    "        name.append(n.text)\n",
    "    except NoSuchElementException:\n",
    "        name.append(\"-\")\n",
    "        \n",
    "    try:\n",
    "        c=driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[9]/div[5]/div/div[2]/div[1]/div[1]/table/tbody/tr[4]/td[2]/ul/li')      # for colour\n",
    "        colour.append(c.text)\n",
    "    except NoSuchElementException:\n",
    "        colour.append(\"-\")\n",
    "         \n",
    "    try:\n",
    "        r=driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[9]/div[5]/div/div[2]/div[1]/div[4]/table/tbody/tr[2]/td[2]/ul/li')            #for ram \n",
    "        ram.append(r.text)\n",
    "    except NoSuchElementException:\n",
    "        ram.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        ro=driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[9]/div[5]/div/div[2]/div[1]/div[4]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        rom.append(ro.text)\n",
    "    except NoSuchElementException:\n",
    "        rom.append(\"-\")\n",
    "    \n",
    "    try:\n",
    "        pc=driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[9]/div[5]/div/div[2]/div[1]/div[5]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        pcam.append(pc.text)\n",
    "    except NoSuchElementException:\n",
    "        pcam.append(\"-\")\n",
    "    try:\n",
    "        s=driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[9]/div[5]/div/div[2]/div[1]/div[5]/table/tbody/tr[5]/td[2]/ul/li')\n",
    "        scam.append(s.text)\n",
    "    except NoSuchElementException:\n",
    "        scam.append(\"-\")\n",
    "    try:\n",
    "        d=driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[9]/div[5]/div/div[2]/div[1]/div[2]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        dsize.append(d.text)\n",
    "    except NoSuchElementException:\n",
    "        dsize.append(\"-\")\n",
    "    try:\n",
    "        ba=driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[9]/div[5]/div/div[2]/div[1]/div[9]/table/tbody/tr/td[2]/ul/li')\n",
    "        battery.append(ba.text)\n",
    "    except NoSuchElementException:\n",
    "        battery.append(\"-\")\n",
    "    try:\n",
    "        pr=driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]')\n",
    "        price.append(pr.text)\n",
    "    except NoSuchElementException:\n",
    "        [price].append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0dc3619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(brand))                     #Here I have printed the lenth of each of the data extracted so that we don't face any problem in creating a DataFrame.\n",
    "print(len(name))\n",
    "print(len(colour))\n",
    "print(len(ram))\n",
    "print(len(rom))\n",
    "print(len(pcam))\n",
    "print(len(scam))\n",
    "print(len(dsize))\n",
    "print(len(battery))\n",
    "print(len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99b30a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Name of the Product</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Rom</th>\n",
       "      <th>Primary Cam</th>\n",
       "      <th>Secondary Cam</th>\n",
       "      <th>Display size</th>\n",
       "      <th>Battery</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realme</td>\n",
       "      <td>realme Narzo 50i (Mint Green, 32 GB)  (2 GB RAM)</td>\n",
       "      <td>Mint Green</td>\n",
       "      <td>2 GB</td>\n",
       "      <td>32 GB</td>\n",
       "      <td>8MP Rear Camera</td>\n",
       "      <td>5MP Front Camera</td>\n",
       "      <td>16.51 cm (6.5 inch)</td>\n",
       "      <td>5000 mAh</td>\n",
       "      <td>₹7,499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realme</td>\n",
       "      <td>realme Narzo 50i (Carbon Black, 32 GB)  (2 GB ...</td>\n",
       "      <td>Carbon Black</td>\n",
       "      <td>2 GB</td>\n",
       "      <td>32 GB</td>\n",
       "      <td>8MP Rear Camera</td>\n",
       "      <td>5MP Front Camera</td>\n",
       "      <td>16.51 cm (6.5 inch)</td>\n",
       "      <td>5000 mAh</td>\n",
       "      <td>₹7,499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realme</td>\n",
       "      <td>realme Narzo 50i (Mint Green, 64 GB)  (4 GB RAM)</td>\n",
       "      <td>Mint Green</td>\n",
       "      <td>4 GB</td>\n",
       "      <td>64 GB</td>\n",
       "      <td>8MP Rear Camera</td>\n",
       "      <td>5MP Front Camera</td>\n",
       "      <td>16.51 cm (6.5 inch)</td>\n",
       "      <td>5000 mAh</td>\n",
       "      <td>₹8,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realme</td>\n",
       "      <td>realme Narzo 50i (Carbon Black, 64 GB)  (4 GB ...</td>\n",
       "      <td>Carbon Black</td>\n",
       "      <td>4 GB</td>\n",
       "      <td>64 GB</td>\n",
       "      <td>8MP Rear Camera</td>\n",
       "      <td>5MP Front Camera</td>\n",
       "      <td>16.51 cm (6.5 inch)</td>\n",
       "      <td>5000 mAh</td>\n",
       "      <td>₹8,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realme</td>\n",
       "      <td>realme Narzo 50A (Oxygen Blue, 64 GB)  (4 GB RAM)</td>\n",
       "      <td>Oxygen Blue</td>\n",
       "      <td>4 GB</td>\n",
       "      <td>64 GB</td>\n",
       "      <td>50MP + 2MP + 2MP</td>\n",
       "      <td>8MP Front Camera</td>\n",
       "      <td>16.51 cm (6.5 inch)</td>\n",
       "      <td>6000 mAh</td>\n",
       "      <td>₹11,499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Brand Name                                Name of the Product        Colour  \\\n",
       "0     realme   realme Narzo 50i (Mint Green, 32 GB)  (2 GB RAM)    Mint Green   \n",
       "1     realme  realme Narzo 50i (Carbon Black, 32 GB)  (2 GB ...  Carbon Black   \n",
       "2     realme   realme Narzo 50i (Mint Green, 64 GB)  (4 GB RAM)    Mint Green   \n",
       "3     realme  realme Narzo 50i (Carbon Black, 64 GB)  (4 GB ...  Carbon Black   \n",
       "4     realme  realme Narzo 50A (Oxygen Blue, 64 GB)  (4 GB RAM)   Oxygen Blue   \n",
       "\n",
       "    Ram    Rom       Primary Cam     Secondary Cam         Display size  \\\n",
       "0  2 GB  32 GB   8MP Rear Camera  5MP Front Camera  16.51 cm (6.5 inch)   \n",
       "1  2 GB  32 GB   8MP Rear Camera  5MP Front Camera  16.51 cm (6.5 inch)   \n",
       "2  4 GB  64 GB   8MP Rear Camera  5MP Front Camera  16.51 cm (6.5 inch)   \n",
       "3  4 GB  64 GB   8MP Rear Camera  5MP Front Camera  16.51 cm (6.5 inch)   \n",
       "4  4 GB  64 GB  50MP + 2MP + 2MP  8MP Front Camera  16.51 cm (6.5 inch)   \n",
       "\n",
       "    Battery    Price  \n",
       "0  5000 mAh   ₹7,499  \n",
       "1  5000 mAh   ₹7,499  \n",
       "2  5000 mAh   ₹8,999  \n",
       "3  5000 mAh   ₹8,999  \n",
       "4  6000 mAh  ₹11,499  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the DataFrame - \n",
    "phone_detail = pd.DataFrame({'Brand Name':brand,'Name of the Product':name,'Colour':colour,'Ram':ram,'Rom':rom,'Primary Cam':pcam,'Secondary Cam':scam,'Display size':dsize,'Battery':battery,'Price':price})\n",
    "phone_detail.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2986961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame as csv file for our future use.\n",
    "phone_detail.to_csv(\"Smartphone Details.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbf47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f368631",
   "metadata": {},
   "source": [
    "### Q5\n",
    " Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google\n",
    "maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534cad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=\"https://www.google.co.in/maps\"\n",
    "driver.get(url)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e772c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding web element for search using id\n",
    "gmap=driver.find_element_by_class_name(\"tactile-searchbox-input\")\n",
    "gmap\n",
    "# writing on search bar\n",
    "gmap.send_keys(\"tokyo\")\n",
    "\n",
    "# click button using xpath\n",
    "search_btn=driver.find_element_by_class_name(\"nhb85d-BIqFsb\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc594a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL Extracted:  https://www.google.co.in/maps/place/Tokyo,+Japan/@35.4884661,137.5260993,7z/data=!3m1!4b1!4m5!3m4!1s0x605d1b87f02e57e7:0x2e01618b22571b89!8m2!3d35.6761919!4d139.6503106\n",
      "Latitude = 35.4884661, \n",
      "Longitude = 137.5260993\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, \\nLongitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6576111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64912b38",
   "metadata": {},
   "source": [
    "## Q6\n",
    "Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) \n",
    "from trak.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2469f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")              \n",
    "url = 'https://trak.in/' # website url\n",
    "driver.get(url)\n",
    "time.sleep(2)  \n",
    "fund = driver.find_element_by_xpath('/html/body/div[1]/header/div[2]/div/div/div/div/nav/ul/li[9]/a')\n",
    "fund.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "678ed383",
   "metadata": {},
   "outputs": [],
   "source": [
    "f= driver.find_element_by_xpath('/html/body/div[1]/header/div[2]/div/div/div/div/nav/ul/li[9]/a')\n",
    "f.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e4a4aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "serial = []                   #Empty List for the data we want to extract\n",
    "date = []\n",
    "startup = []\n",
    "industry = []\n",
    "sub = []\n",
    "city = []\n",
    "investment = []\n",
    "typ = []\n",
    "amount = []\n",
    "\n",
    "s = driver.find_elements_by_xpath('//*[@id=\"tablepress-54\"]/tbody/tr/td[1]')  #Extracting the table data by the xpath.\n",
    "d = driver.find_elements_by_xpath('//*[@id=\"tablepress-54\"]/tbody/tr/td[2]')\n",
    "st = driver.find_elements_by_xpath('//*[@id=\"tablepress-54\"]/tbody/tr/td[3]')\n",
    "ind = driver.find_elements_by_xpath('//*[@id=\"tablepress-54\"]/tbody/tr/td[4]')\n",
    "su = driver.find_elements_by_xpath('//*[@id=\"tablepress-54\"]/tbody/tr/td[5]')\n",
    "c = driver.find_elements_by_xpath('//*[@id=\"tablepress-54\"]/tbody/tr/td[6]')\n",
    "inv = driver.find_elements_by_xpath('//*[@id=\"tablepress-54\"]/tbody/tr/td[7]')\n",
    "t = driver.find_elements_by_xpath('//*[@id=\"tablepress-54\"]/tbody/tr/td[8]')\n",
    "a = driver.find_elements_by_xpath('//*[@id=\"tablepress-54\"]/tbody/tr/td[9]')\n",
    "\n",
    "\n",
    "for i in s:\n",
    "    serial.append(i.text)    #Appending the text data.\n",
    "for j in d:\n",
    "    date.append(j.text)\n",
    "for k in st:\n",
    "    startup.append(k.text)\n",
    "for l in ind:\n",
    "    industry.append(l.text)\n",
    "for m in su:\n",
    "    sub.append(m.text)\n",
    "for n in c:\n",
    "    city.append(n.text)\n",
    "for o in inv:\n",
    "    investment.append(o.text)\n",
    "for p in t:\n",
    "    typ.append(p.text)\n",
    "for q in a:\n",
    "    amount.append(q.text)\n",
    "    \n",
    "print(len(serial))     # Printing the length of each of the Data Extracted so that while creating the DataFrame we don't face any problem.\n",
    "print(len(date))\n",
    "print(len(startup))\n",
    "print(len(industry))\n",
    "print(len(sub))\n",
    "print(len(city))\n",
    "print(len(investment))\n",
    "print(len(typ))\n",
    "print(len(amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8765147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "january=pd.DataFrame({'serial':serial,'Date':date,'Startup':startup,'Industry':industry,'Sub-Vertical':sub,'City':city,'Investment':investment,'Type':typ,'Amount':amount})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d72c4b",
   "metadata": {},
   "source": [
    "### Funding Deal (January-21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc277599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>Date</th>\n",
       "      <th>Startup</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sub-Vertical</th>\n",
       "      <th>City</th>\n",
       "      <th>Investment</th>\n",
       "      <th>Type</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15/01/2021</td>\n",
       "      <td>Digit Insurance</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Insurance Services</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>A91 Partners, Faering Capital, TVS Capital Funds</td>\n",
       "      <td>Venture</td>\n",
       "      <td>1,80,00,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>28/01/2021</td>\n",
       "      <td>Bombay Shaving Company</td>\n",
       "      <td>Consumer Goods Company</td>\n",
       "      <td>Shave care, beard care, and skincare products</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Reckitt Benckiser</td>\n",
       "      <td>Venture</td>\n",
       "      <td>6,172,258.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>19/01/2021</td>\n",
       "      <td>DeHaat</td>\n",
       "      <td>AgriTech Startup</td>\n",
       "      <td>online marketplace for farm products and services</td>\n",
       "      <td>Patna</td>\n",
       "      <td>Prosus Ventures</td>\n",
       "      <td>Series C</td>\n",
       "      <td>30,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>19/01/2021</td>\n",
       "      <td>Darwinbox</td>\n",
       "      <td>SaaS</td>\n",
       "      <td>HR Tech</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Salesforce Ventures</td>\n",
       "      <td>Seed</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>18/01/2021</td>\n",
       "      <td>mfine</td>\n",
       "      <td>Health Tech Startup</td>\n",
       "      <td>AI-powered telemedicine mobile app</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Heritas Capital Management</td>\n",
       "      <td>Venture Round</td>\n",
       "      <td>16,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>18/01/2021</td>\n",
       "      <td>Udayy</td>\n",
       "      <td>EdTech</td>\n",
       "      <td>Online learning platform for kids in class 1-5</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Sequoia Capital</td>\n",
       "      <td>Seed Funding</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>11/01/2021</td>\n",
       "      <td>True Elements</td>\n",
       "      <td>Food Startup</td>\n",
       "      <td>Whole Food plant based Nashta</td>\n",
       "      <td>Pune</td>\n",
       "      <td>SIDBI Venture Capital</td>\n",
       "      <td>Series</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>13/01/2021</td>\n",
       "      <td>Saveo</td>\n",
       "      <td>B2B E-commerce</td>\n",
       "      <td>Pharmacies</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Matrix Partners India, RTP Global, others</td>\n",
       "      <td>Seed</td>\n",
       "      <td>4,000,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  serial        Date                 Startup                Industry  \\\n",
       "0      1  15/01/2021         Digit Insurance      Financial Services   \n",
       "1      2  28/01/2021  Bombay Shaving Company  Consumer Goods Company   \n",
       "2      3  19/01/2021                  DeHaat        AgriTech Startup   \n",
       "3      4  19/01/2021               Darwinbox                    SaaS   \n",
       "4      5  18/01/2021                   mfine     Health Tech Startup   \n",
       "5      6  18/01/2021                   Udayy                  EdTech   \n",
       "6      7  11/01/2021           True Elements            Food Startup   \n",
       "7      8  13/01/2021                   Saveo          B2B E-commerce   \n",
       "\n",
       "                                        Sub-Vertical       City  \\\n",
       "0                                 Insurance Services  Bengaluru   \n",
       "1      Shave care, beard care, and skincare products  New Delhi   \n",
       "2  online marketplace for farm products and services      Patna   \n",
       "3                                            HR Tech     Mumbai   \n",
       "4                 AI-powered telemedicine mobile app  Bengaluru   \n",
       "5     Online learning platform for kids in class 1-5    Gurgaon   \n",
       "6                      Whole Food plant based Nashta       Pune   \n",
       "7                                         Pharmacies  Bengaluru   \n",
       "\n",
       "                                         Investment           Type  \\\n",
       "0  A91 Partners, Faering Capital, TVS Capital Funds        Venture   \n",
       "1                                 Reckitt Benckiser        Venture   \n",
       "2                                   Prosus Ventures       Series C   \n",
       "3                               Salesforce Ventures           Seed   \n",
       "4                        Heritas Capital Management  Venture Round   \n",
       "5                                   Sequoia Capital   Seed Funding   \n",
       "6                             SIDBI Venture Capital         Series   \n",
       "7         Matrix Partners India, RTP Global, others           Seed   \n",
       "\n",
       "         Amount  \n",
       "0   1,80,00,000  \n",
       "1  6,172,258.50  \n",
       "2    30,000,000  \n",
       "3    15,000,000  \n",
       "4    16,000,000  \n",
       "5    15,000,000  \n",
       "6   100,000,000  \n",
       "7     4,000,000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "january"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4dd8bff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "serial = []                   #Empty List for the data we want to extract\n",
    "date = []\n",
    "startup = []\n",
    "industry = []\n",
    "sub = []\n",
    "city = []\n",
    "investment = []\n",
    "typ = []\n",
    "amount = []\n",
    "\n",
    "s = driver.find_elements_by_xpath('//*[@id=\"tablepress-55\"]/tbody/tr/td[1]')  #Extracting the table data by the xpath.\n",
    "d = driver.find_elements_by_xpath('//*[@id=\"tablepress-55\"]/tbody/tr/td[2]')\n",
    "st = driver.find_elements_by_xpath('//*[@id=\"tablepress-55\"]/tbody/tr/td[3]')\n",
    "ind = driver.find_elements_by_xpath('//*[@id=\"tablepress-55\"]/tbody/tr/td[4]')\n",
    "su = driver.find_elements_by_xpath('//*[@id=\"tablepress-55\"]/tbody/tr/td[5]')\n",
    "c = driver.find_elements_by_xpath('//*[@id=\"tablepress-55\"]/tbody/tr/td[6]')\n",
    "inv = driver.find_elements_by_xpath('//*[@id=\"tablepress-55\"]/tbody/tr/td[7]')\n",
    "t = driver.find_elements_by_xpath('//*[@id=\"tablepress-55\"]/tbody/tr/td[8]')\n",
    "a = driver.find_elements_by_xpath('//*[@id=\"tablepress-55\"]/tbody/tr/td[9]')\n",
    "\n",
    "\n",
    "for i in s:\n",
    "    serial.append(i.text)    #Appending the text data.\n",
    "for j in d:\n",
    "    date.append(j.text)\n",
    "for k in st:\n",
    "    startup.append(k.text)\n",
    "for l in ind:\n",
    "    industry.append(l.text)\n",
    "for m in su:\n",
    "    sub.append(m.text)\n",
    "for n in c:\n",
    "    city.append(n.text)\n",
    "for o in inv:\n",
    "    investment.append(o.text)\n",
    "for p in t:\n",
    "    typ.append(p.text)\n",
    "for q in a:\n",
    "    amount.append(q.text)\n",
    "    \n",
    "print(len(serial))     # Printing the length of each of the Data Extracted so that while creating the DataFrame we don't face any problem.\n",
    "print(len(date))\n",
    "print(len(startup))\n",
    "print(len(industry))\n",
    "print(len(sub))\n",
    "print(len(city))\n",
    "print(len(investment))\n",
    "print(len(typ))\n",
    "print(len(amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2e0fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "February=pd.DataFrame({'serial':serial,'Date':date,'Startup':startup,'Industry':industry,'Sub-Vertical':sub,'City':city,'Investment':investment,'Type':typ,'Amount':amount})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65746a",
   "metadata": {},
   "source": [
    "### Funding Deal (February-21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "968b9bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>Date</th>\n",
       "      <th>Startup</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sub-Vertical</th>\n",
       "      <th>City</th>\n",
       "      <th>Investment</th>\n",
       "      <th>Type</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11/02/2021</td>\n",
       "      <td>Doubtnut</td>\n",
       "      <td>Edu Tech</td>\n",
       "      <td>E-Learning Platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>SIG Global, Sequoia Capital, WaterBridge Ventu...</td>\n",
       "      <td>Series B</td>\n",
       "      <td>2,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22/02/2021</td>\n",
       "      <td>Zomato</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>Online Food Delivery Platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Tiger Global, Kora</td>\n",
       "      <td>Venture</td>\n",
       "      <td>250,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>19/02/2021</td>\n",
       "      <td>Fingerlix</td>\n",
       "      <td>Hospitality</td>\n",
       "      <td>Semi-cooked food delivery app</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Rhodium Trust, Accel Partners and Swiggy</td>\n",
       "      <td>Series C</td>\n",
       "      <td>2,747,045.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17/02/2021</td>\n",
       "      <td>Zolve</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Global Neobank Venture</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Accel Partners and Lightspeed Venture Partners</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,50,00,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15/02/2021</td>\n",
       "      <td>KreditBee</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Digital lending platform</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Azim Premji’s PremjiInvest and South Korea’s M...</td>\n",
       "      <td>Series C</td>\n",
       "      <td>75,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>Pepperfry</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Multi-brand furniture brand</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>InnoVen Capital</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>4,773,958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>Grofers</td>\n",
       "      <td>E-Commerce</td>\n",
       "      <td>Online supermarket</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>SoftBank Vision Fund (SVF)</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>55,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>09/02/2021</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Consumer Technology Venture</td>\n",
       "      <td>London</td>\n",
       "      <td>GV</td>\n",
       "      <td>Series A</td>\n",
       "      <td>15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>09/02/2021</td>\n",
       "      <td>SplashLearn</td>\n",
       "      <td>EdTech</td>\n",
       "      <td>Game-based learning programme</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Owl Ventures</td>\n",
       "      <td>Series C</td>\n",
       "      <td>18,000,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  serial        Date      Startup     Industry                   Sub-Vertical  \\\n",
       "0      1  11/02/2021     Doubtnut     Edu Tech            E-Learning Platform   \n",
       "1      2  22/02/2021       Zomato  Hospitality  Online Food Delivery Platform   \n",
       "2      3  19/02/2021    Fingerlix  Hospitality  Semi-cooked food delivery app   \n",
       "3      4  17/02/2021        Zolve      FinTech         Global Neobank Venture   \n",
       "4      5  15/02/2021    KreditBee      Finance       Digital lending platform   \n",
       "5      6  12/02/2021    Pepperfry   E-commerce    Multi-brand furniture brand   \n",
       "6      7  12/02/2021      Grofers   E-Commerce             Online supermarket   \n",
       "7      8  09/02/2021      Nothing   Technology    Consumer Technology Venture   \n",
       "8      9  09/02/2021  SplashLearn       EdTech  Game-based learning programme   \n",
       "\n",
       "        City                                         Investment  \\\n",
       "0    Gurgaon  SIG Global, Sequoia Capital, WaterBridge Ventu...   \n",
       "1    Gurgaon                                 Tiger Global, Kora   \n",
       "2     Mumbai           Rhodium Trust, Accel Partners and Swiggy   \n",
       "3     Mumbai     Accel Partners and Lightspeed Venture Partners   \n",
       "4  Bengaluru  Azim Premji’s PremjiInvest and South Korea’s M...   \n",
       "5     Mumbai                                    InnoVen Capital   \n",
       "6    Gurgaon                         SoftBank Vision Fund (SVF)   \n",
       "7     London                                                 GV   \n",
       "8    Gurgaon                                       Owl Ventures   \n",
       "\n",
       "             Type        Amount  \n",
       "0        Series B     2,500,000  \n",
       "1         Venture   250,000,000  \n",
       "2        Series C  2,747,045.20  \n",
       "3            Seed   1,50,00,000  \n",
       "4        Series C    75,000,000  \n",
       "5  Debt Financing     4,773,958  \n",
       "6     Unspecified    55,000,000  \n",
       "7        Series A    15,000,000  \n",
       "8        Series C    18,000,000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bee980f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "serial = []                   #Empty List for the data we want to extract\n",
    "date = []\n",
    "startup = []\n",
    "industry = []\n",
    "sub = []\n",
    "city = []\n",
    "investment = []\n",
    "typ = []\n",
    "amount = []\n",
    "\n",
    "s = driver.find_elements_by_xpath('//*[@id=\"tablepress-56\"]/tbody/tr/td[1]')  #Extracting the table data by the xpath.\n",
    "d = driver.find_elements_by_xpath('//*[@id=\"tablepress-56\"]/tbody/tr/td[2]')\n",
    "st = driver.find_elements_by_xpath('//*[@id=\"tablepress-56\"]/tbody/tr/td[3]')\n",
    "ind = driver.find_elements_by_xpath('//*[@id=\"tablepress-56\"]/tbody/tr/td[4]')\n",
    "su = driver.find_elements_by_xpath('//*[@id=\"tablepress-56\"]/tbody/tr/td[5]')\n",
    "c = driver.find_elements_by_xpath('//*[@id=\"tablepress-56\"]/tbody/tr/td[6]')\n",
    "inv = driver.find_elements_by_xpath('//*[@id=\"tablepress-56\"]/tbody/tr/td[7]')\n",
    "t = driver.find_elements_by_xpath('//*[@id=\"tablepress-56\"]/tbody/tr/td[8]')\n",
    "a = driver.find_elements_by_xpath('//*[@id=\"tablepress-56\"]/tbody/tr/td[9]')\n",
    "\n",
    "\n",
    "for i in s:\n",
    "    serial.append(i.text)    #Appending the text data.\n",
    "for j in d:\n",
    "    date.append(j.text)\n",
    "for k in st:\n",
    "    startup.append(k.text)\n",
    "for l in ind:\n",
    "    industry.append(l.text)\n",
    "for m in su:\n",
    "    sub.append(m.text)\n",
    "for n in c:\n",
    "    city.append(n.text)\n",
    "for o in inv:\n",
    "    investment.append(o.text)\n",
    "for p in t:\n",
    "    typ.append(p.text)\n",
    "for q in a:\n",
    "    amount.append(q.text)\n",
    "    \n",
    "print(len(serial))     # Printing the length of each of the Data Extracted so that while creating the DataFrame we don't face any problem.\n",
    "print(len(date))\n",
    "print(len(startup))\n",
    "print(len(industry))\n",
    "print(len(sub))\n",
    "print(len(city))\n",
    "print(len(investment))\n",
    "print(len(typ))\n",
    "print(len(amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff995db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "march=pd.DataFrame({'serial':serial,'Date':date,'Startup':startup,'Industry':industry,'Sub-Vertical':sub,'City':city,'Investment':investment,'Type':typ,'Amount':amount})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea68752",
   "metadata": {},
   "source": [
    "### Funding Deal (March-21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "633583aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>Date</th>\n",
       "      <th>Startup</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sub-Vertical</th>\n",
       "      <th>City</th>\n",
       "      <th>Investment</th>\n",
       "      <th>Type</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>04/03/2021</td>\n",
       "      <td>DealShare</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online shopping platform</td>\n",
       "      <td>Jaipur, Rajasthan</td>\n",
       "      <td>Innoven Capital</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>250,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>31/03/2021</td>\n",
       "      <td>Uniphore</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Conversational Service Automation (CSA)</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Sorenson Capital Partners</td>\n",
       "      <td>Series D</td>\n",
       "      <td>140,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>30/03/2021</td>\n",
       "      <td>Dunzo</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Hyper-local delivery app</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Krishtal Advisors Pte Ltd</td>\n",
       "      <td>Series E</td>\n",
       "      <td>8,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>30/03/2021</td>\n",
       "      <td>BYJU’S</td>\n",
       "      <td>Edu-tech</td>\n",
       "      <td>Online tutoring</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>MC Global Edtech, B Capital, Baron, others</td>\n",
       "      <td>Series F</td>\n",
       "      <td>460,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>23/03/2021</td>\n",
       "      <td>SkilloVilla</td>\n",
       "      <td>Edu-tech</td>\n",
       "      <td>Career and job-oriented upskilling.</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Titan Capital, others</td>\n",
       "      <td>Seed</td>\n",
       "      <td>300,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>25/03/2021</td>\n",
       "      <td>CityMall</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Social ecommerce and online grocery platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Accel Partners</td>\n",
       "      <td>Series A</td>\n",
       "      <td>11,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>26/03/2021</td>\n",
       "      <td>DotPe</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Commerce and payments platform to offline ente...</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>PayU</td>\n",
       "      <td>Series A</td>\n",
       "      <td>27,500,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  serial        Date      Startup    Industry  \\\n",
       "0      1  04/03/2021    DealShare  E-commerce   \n",
       "1      2  31/03/2021     Uniphore  Technology   \n",
       "2      3  30/03/2021        Dunzo  E-commerce   \n",
       "3      4  30/03/2021       BYJU’S    Edu-tech   \n",
       "4      5  23/03/2021  SkilloVilla    Edu-tech   \n",
       "5      6  25/03/2021     CityMall  E-commerce   \n",
       "6      7  26/03/2021        DotPe     FinTech   \n",
       "\n",
       "                                        Sub-Vertical               City  \\\n",
       "0                           Online shopping platform  Jaipur, Rajasthan   \n",
       "1            Conversational Service Automation (CSA)          Palo Alto   \n",
       "2                           Hyper-local delivery app          Bengaluru   \n",
       "3                                    Online tutoring          Bengaluru   \n",
       "4                Career and job-oriented upskilling.          Bengaluru   \n",
       "5       Social ecommerce and online grocery platform            Gurgaon   \n",
       "6  Commerce and payments platform to offline ente...            Gurgaon   \n",
       "\n",
       "                                   Investment            Type       Amount  \n",
       "0                             Innoven Capital  Debt Financing  250,000,000  \n",
       "1                   Sorenson Capital Partners        Series D  140,000,000  \n",
       "2                   Krishtal Advisors Pte Ltd        Series E    8,000,000  \n",
       "3  MC Global Edtech, B Capital, Baron, others        Series F  460,000,000  \n",
       "4                       Titan Capital, others            Seed  300,000,000  \n",
       "5                              Accel Partners        Series A   11,000,000  \n",
       "6                                        PayU        Series A   27,500,000  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "march"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ad5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c80b6bb",
   "metadata": {},
   "source": [
    "## Q7\n",
    "Write a program to scrap all the available details of best gaming laptops from digit.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4885903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")              \n",
    "url = 'https://www.digit.in/' # website url\n",
    "driver.get(url)\n",
    "time.sleep(2)  \n",
    "search = driver.find_element_by_xpath('/html/body/div[4]/div/div[2]/div[2]/div[4]/ul/li[9]/a') \n",
    "search.click()                # clicking on best gaming laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4aa8c323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "name = []                   #Empty List for the data we want to extract\n",
    "os = []\n",
    "disp = []\n",
    "pro = []\n",
    "mem = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = driver.find_elements_by_tag_name('h3')   #Extracting the table data by the xpath.\n",
    "o = driver.find_elements_by_xpath('//div[@class=\"product-detail\"]/div/ul/li[1]/div')\n",
    "d = driver.find_elements_by_xpath('//div[@class=\"product-detail\"]/div/ul/li[2]/div')\n",
    "p = driver.find_elements_by_xpath('//div[@class=\"product-detail\"]/div/ul/li[4]/div')\n",
    "mm = driver.find_elements_by_xpath('//div[@class=\"product-detail\"]/div/ul/li[5]/div')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in n:\n",
    "    name.append(i.text)    #Appending the text data.\n",
    "for j in o:\n",
    "    os.append(j.text.split('\\n')[1])\n",
    "for k in d:\n",
    "    disp.append(k.text.split('\\n')[1])\n",
    "for l in p:\n",
    "    pro.append(l.text.split('\\n')[1])\n",
    "for m in mm:\n",
    "    mem.append(m.text.split('\\n')[1])\n",
    "prices=driver.find_elements_by_xpath(\"//td[@class='smprice']\")\n",
    "for n in pp:\n",
    "    if n.text is NoSuchElementException:\n",
    "        pr.append('-')\n",
    "    else:\n",
    "        pr.append(n.text)\n",
    "       \n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "print(len(name))     # Printing the length of each of the Data Extracted so that while creating the DataFrame we don't face any problem.\n",
    "print(len(os))\n",
    "print(len(disp))\n",
    "print(len(pro))\n",
    "print(len(mem))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "873f7bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>OS</th>\n",
       "      <th>Display</th>\n",
       "      <th>Processor</th>\n",
       "      <th>Memory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACER NITRO 5</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>AMD RYZEN 9 OCTA CORE | 2.4 GHZ</td>\n",
       "      <td>1 TB HDD/16 GBGB DDR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSI STEALTH 15M</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>INTEL CORE I7 11TH GEN - 11375H | NA</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASUS ROG STRIX SCAR 15</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (2560 X 1440)</td>\n",
       "      <td>AMD RYZEN 9 OCTA CORE - 5900HX | 3.3 GHZ</td>\n",
       "      <td>2 TB SSD/32 GBGB DDR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALIENWARE AREA 51M R2</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>17.3\" (1920 X 1080)</td>\n",
       "      <td>10TH GEN INTEL® CORE™ I7-10700 | 2.90 GHZ</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALIENWARE M15 R3</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>15.6\" (3840 X 2160)</td>\n",
       "      <td>10TH GEN INTEL® CORE™ I9-10980HK | NA</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS ROG STRIX SCAR 15</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>AMD RYZEN™ 9 5900HX | 3.3 GHZ</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS ZEPHYRUS G14</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>14\" (1920 X 1080)</td>\n",
       "      <td>AMD 3RD GEN RYZEN 9 | 3.3 GHZ</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LENOVO LEGION 5I</td>\n",
       "      <td>WINDOWS 10 PRO</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>10TH GEN INTEL® CORE™ I5-10300H | 2.50 GHZ</td>\n",
       "      <td>1 TB SSD/16 GBGB DDR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASUS ROG ZEPHYRUS DUO 15</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (3840 X 1100)</td>\n",
       "      <td>INTEL CORE I7 10TH GEN 10875H | NA</td>\n",
       "      <td>512 GB SSD/4 GBGB DDR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACER ASPIRE 7 GAMING LAPTOP</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>AMD RYZEN™ 5-5500U HEXA-CORE | NA</td>\n",
       "      <td>512 GB SSD/8 GBGB DDR4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Name               OS              Display  \\\n",
       "0                 ACER NITRO 5       WINDOWS 10  15.6\" (1920 X 1080)   \n",
       "1              MSI STEALTH 15M       WINDOWS 10  15.6\" (1920 X 1080)   \n",
       "2       ASUS ROG STRIX SCAR 15       WINDOWS 10  15.6\" (2560 X 1440)   \n",
       "3        ALIENWARE AREA 51M R2  WINDOWS 10 HOME  17.3\" (1920 X 1080)   \n",
       "4             ALIENWARE M15 R3  WINDOWS 10 HOME  15.6\" (3840 X 2160)   \n",
       "5       ASUS ROG STRIX SCAR 15  WINDOWS 10 HOME  15.6\" (1920 X 1080)   \n",
       "6            ASUS ZEPHYRUS G14  WINDOWS 10 HOME    14\" (1920 X 1080)   \n",
       "7             LENOVO LEGION 5I   WINDOWS 10 PRO  15.6\" (1920 X 1080)   \n",
       "8     ASUS ROG ZEPHYRUS DUO 15       WINDOWS 10  15.6\" (3840 X 1100)   \n",
       "9  ACER ASPIRE 7 GAMING LAPTOP  WINDOWS 10 HOME  15.6\" (1920 X 1080)   \n",
       "\n",
       "                                    Processor                  Memory  \n",
       "0             AMD RYZEN 9 OCTA CORE | 2.4 GHZ   1 TB HDD/16 GBGB DDR4  \n",
       "1        INTEL CORE I7 11TH GEN - 11375H | NA   1 TB SSD/16 GBGB DDR4  \n",
       "2    AMD RYZEN 9 OCTA CORE - 5900HX | 3.3 GHZ   2 TB SSD/32 GBGB DDR4  \n",
       "3   10TH GEN INTEL® CORE™ I7-10700 | 2.90 GHZ   1 TB SSD/16 GBGB DDR4  \n",
       "4       10TH GEN INTEL® CORE™ I9-10980HK | NA   1 TB SSD/16 GBGB DDR4  \n",
       "5               AMD RYZEN™ 9 5900HX | 3.3 GHZ   1 TB SSD/16 GBGB DDR4  \n",
       "6               AMD 3RD GEN RYZEN 9 | 3.3 GHZ   1 TB SSD/16 GBGB DDR4  \n",
       "7  10TH GEN INTEL® CORE™ I5-10300H | 2.50 GHZ   1 TB SSD/16 GBGB DDR4  \n",
       "8          INTEL CORE I7 10TH GEN 10875H | NA  512 GB SSD/4 GBGB DDR4  \n",
       "9           AMD RYZEN™ 5-5500U HEXA-CORE | NA  512 GB SSD/8 GBGB DDR4  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe\n",
    "Laptop=pd.DataFrame({'Name':name,'OS':os,'Display':disp,'Processor':pro,'Memory':mem})\n",
    "Laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b3ad6c",
   "metadata": {},
   "source": [
    "## Q8.\n",
    " Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be \n",
    "scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "98f3cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7fef3dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "rank=[]\n",
    "name = []                   #Empty List for the data we want to extract\n",
    "net = []\n",
    "age = []\n",
    "city = []\n",
    "source = []\n",
    "ind=[]\n",
    "\n",
    "rr = driver.find_elements_by_xpath('//div[@class=\"rank\"]') #Extracting the table data by the xpath.\n",
    "nn = driver.find_elements_by_xpath('//div[@class=\"personName\"]')\n",
    "ne = driver.find_elements_by_xpath('//div[@class=\"netWorth\"]')\n",
    "aa = driver.find_elements_by_xpath('//div[@class=\"age\"]')\n",
    "cc = driver.find_elements_by_xpath('//div[@class=\"countryOfCitizenship\"]')\n",
    "ss = driver.find_elements_by_xpath('//div[@class=\"source-column\"]')\n",
    "ii = driver.find_elements_by_xpath('//div[@class=\"category\"]')\n",
    "\n",
    "\n",
    "for i in rr:\n",
    "    rank.append(i.text)    #Appending the text data.\n",
    "for j in nn:\n",
    "    name.append(j.text)\n",
    "for k in ne:\n",
    "    net.append(k.text)\n",
    "for l in aa:\n",
    "    age.append(l.text)\n",
    "for m in cc:\n",
    "    city.append(m.text)\n",
    "for n in ss:\n",
    "    source.append(n.text)\n",
    "for o in ii:\n",
    "    ind.append(o.text)\n",
    "\n",
    "    \n",
    "print(len(rank))     # Printing the length of each of the Data Extracted so that while creating the DataFrame we don't face any problem.\n",
    "print(len(name))\n",
    "print(len(net))\n",
    "print(len(age))\n",
    "print(len(city))\n",
    "print(len(source))\n",
    "print(len(ind))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c5810925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Net- Worth</th>\n",
       "      <th>Age</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>Source</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>$177 B</td>\n",
       "      <td>57</td>\n",
       "      <td>United States</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>$151 B</td>\n",
       "      <td>49</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tesla, SpaceX</td>\n",
       "      <td>Automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Bernard Arnault &amp; family</td>\n",
       "      <td>$150 B</td>\n",
       "      <td>72</td>\n",
       "      <td>France</td>\n",
       "      <td>LVMH</td>\n",
       "      <td>Fashion &amp; Retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>$124 B</td>\n",
       "      <td>65</td>\n",
       "      <td>United States</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>Mark Zuckerberg</td>\n",
       "      <td>$97 B</td>\n",
       "      <td>36</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195.</td>\n",
       "      <td>Harry Triguboff</td>\n",
       "      <td>$11.2 B</td>\n",
       "      <td>88</td>\n",
       "      <td>Australia</td>\n",
       "      <td>real estate</td>\n",
       "      <td>Real Estate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197.</td>\n",
       "      <td>Leonid Fedun &amp; family</td>\n",
       "      <td>$11.1 B</td>\n",
       "      <td>65</td>\n",
       "      <td>Russia</td>\n",
       "      <td>oil</td>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197.</td>\n",
       "      <td>Eyal Ofer</td>\n",
       "      <td>$11.1 B</td>\n",
       "      <td>70</td>\n",
       "      <td>Israel</td>\n",
       "      <td>real estate, shipping</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>197.</td>\n",
       "      <td>Evan Spiegel</td>\n",
       "      <td>$11.1 B</td>\n",
       "      <td>30</td>\n",
       "      <td>United States</td>\n",
       "      <td>Snapchat</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200.</td>\n",
       "      <td>Luis Carlos Sarmiento</td>\n",
       "      <td>$11 B</td>\n",
       "      <td>88</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>banking</td>\n",
       "      <td>Finance &amp; Investments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rank                      Name Net- Worth Age    Citizenship  \\\n",
       "0      1.                Jeff Bezos     $177 B  57  United States   \n",
       "1      2.                 Elon Musk     $151 B  49  United States   \n",
       "2      3.  Bernard Arnault & family     $150 B  72         France   \n",
       "3      4.                Bill Gates     $124 B  65  United States   \n",
       "4      5.           Mark Zuckerberg      $97 B  36  United States   \n",
       "..    ...                       ...        ...  ..            ...   \n",
       "195  195.           Harry Triguboff    $11.2 B  88      Australia   \n",
       "196  197.     Leonid Fedun & family    $11.1 B  65         Russia   \n",
       "197  197.                 Eyal Ofer    $11.1 B  70         Israel   \n",
       "198  197.              Evan Spiegel    $11.1 B  30  United States   \n",
       "199  200.     Luis Carlos Sarmiento      $11 B  88       Colombia   \n",
       "\n",
       "                    Source               Industry  \n",
       "0                   Amazon             Technology  \n",
       "1            Tesla, SpaceX             Automotive  \n",
       "2                     LVMH       Fashion & Retail  \n",
       "3                Microsoft             Technology  \n",
       "4                 Facebook             Technology  \n",
       "..                     ...                    ...  \n",
       "195            real estate            Real Estate  \n",
       "196                    oil                 Energy  \n",
       "197  real estate, shipping            Diversified  \n",
       "198               Snapchat             Technology  \n",
       "199                banking  Finance & Investments  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe\n",
    "Billionaires=pd.DataFrame({'Rank':rank,'Name':name,'Net- Worth':net,'Age':age,'Citizenship':city,'Source':source,'Industry':ind})\n",
    "Billionaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00820a",
   "metadata": {},
   "source": [
    "## Q9\n",
    " Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted \n",
    "from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f78421fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numb\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.youtube.com/' #opening the website\n",
    "driver.get(url)\n",
    "\n",
    "search = driver.find_element_by_xpath(\"/html/body/ytd-app/div/div/ytd-masthead/div[3]/div[2]/ytd-searchbox/form/div[1]/div[1]/input\") # locating search bar\n",
    "time.sleep(2)\n",
    "search.send_keys(input())    # Asking the User to input which youtube video he/she wants's to enter in search bar.\n",
    "\n",
    "button = driver.find_element_by_id('search-icon-legacy')        # locating search button\n",
    "button.click()                                                  # clicking search button\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "url=[]                                                         # Extracting the url of very First youtube video \n",
    "fir=driver.find_elements_by_xpath('//h3[@class=\"title-and-badge style-scope ytd-video-renderer\"]/a')\n",
    "for i in fir[0:1]:\n",
    "    url.append(i.get_attribute('href'))             # href for 1st video\n",
    "url\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "for i in url:                                                  \n",
    "    driver.get(i)                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "458758a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "for j in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,500)\")           # Scrolling that particular Youtube video to the last so, that all the comment's,likes and uplikes get's visible. \n",
    "time.sleep(5)\n",
    "com = []                                                      # Giving empty list for all the data what all I want to extract \n",
    "upvote=[]\n",
    "time=[]\n",
    "comm =driver.find_elements_by_xpath('//*[@id=\"content-text\"]')  # Giving the xpath of the desired element.\n",
    "vote=driver.find_elements_by_id('vote-count-middle')\n",
    "ti=driver.find_elements_by_id('header-author')\n",
    "for k in comm:\n",
    "    com.append(k.text.replace('\\n',\"\"))           # Appending the text data and replacing the unproductive data by \"\\n\"\n",
    "for l in vote:\n",
    "    upvote.append(l.text)\n",
    "for m in ti:\n",
    "    time.append(m.text.split('\\n')[-1])\n",
    "print(len(com))                                   # Printing the length\n",
    "print(len(upvote))\n",
    "print(len(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "32f1b32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Upvote</th>\n",
       "      <th>Time of comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Next 10 years I’ll still enjoy this son...</td>\n",
       "      <td>916</td>\n",
       "      <td>1 year ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linkin Park songs have always been part of the...</td>\n",
       "      <td>795</td>\n",
       "      <td>9 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siempre fue un grande, descansa en paz Chester...</td>\n",
       "      <td>9</td>\n",
       "      <td>17 hours ago (edited)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is listening to this amazing music in 2022 ?</td>\n",
       "      <td>5.1K</td>\n",
       "      <td>3 weeks ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Essa música me trás uma nostalgia incrível, nã...</td>\n",
       "      <td>10</td>\n",
       "      <td>12 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>If you're reading this, you're a legendR.I.P C...</td>\n",
       "      <td>47</td>\n",
       "      <td>3 weeks ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>A vibe dessa música</td>\n",
       "      <td>2</td>\n",
       "      <td>1 month ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>My parents thinks that I’m listening to this s...</td>\n",
       "      <td>1</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>LP is the only band whose every song is relata...</td>\n",
       "      <td>335</td>\n",
       "      <td>8 months ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1,7 bilhões essa música é uma lenda link park ...</td>\n",
       "      <td>51</td>\n",
       "      <td>6 days ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Comment Upvote  \\\n",
       "0   In the Next 10 years I’ll still enjoy this son...    916   \n",
       "1   Linkin Park songs have always been part of the...    795   \n",
       "2   Siempre fue un grande, descansa en paz Chester...      9   \n",
       "3    Who is listening to this amazing music in 2022 ?   5.1K   \n",
       "4   Essa música me trás uma nostalgia incrível, nã...     10   \n",
       "..                                                ...    ...   \n",
       "75  If you're reading this, you're a legendR.I.P C...     47   \n",
       "76                                A vibe dessa música      2   \n",
       "77  My parents thinks that I’m listening to this s...      1   \n",
       "78  LP is the only band whose every song is relata...    335   \n",
       "79  1,7 bilhões essa música é uma lenda link park ...     51   \n",
       "\n",
       "          Time of comment  \n",
       "0              1 year ago  \n",
       "1              9 days ago  \n",
       "2   17 hours ago (edited)  \n",
       "3             3 weeks ago  \n",
       "4            12 hours ago  \n",
       "..                    ...  \n",
       "75            3 weeks ago  \n",
       "76            1 month ago  \n",
       "77             2 days ago  \n",
       "78           8 months ago  \n",
       "79             6 days ago  \n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe\n",
    "youtube=pd.DataFrame({'Comment':com,'Upvote':upvote,'Time of comment':time})\n",
    "youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c858153e",
   "metadata": {},
   "source": [
    "## Q10\n",
    "Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in \n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, \n",
    "overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a430b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "url = 'https://www.hostelworld.com/'  # Opeing the url\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "london=driver.find_element_by_xpath('/html/body/div[3]/div/div/div[1]/div[7]/section/div/div[1]/a') # Clicking on london location\n",
    "london.click()\n",
    "time.sleep(5)\n",
    "search = driver.find_element_by_xpath('/html/body/div[1]/div[1]/div[2]/section[1]/div/div/form/div/div[2]/button') # Clicking on search button to search all the hostel's in London location\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "172ce6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "154\n",
      "154\n",
      "77\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "name = []                        # Empty List\n",
    "distance=[]\n",
    "ratings=[]\n",
    "reviews=[]\n",
    "treview=[]\n",
    "private=[]\n",
    "dorms=[]\n",
    "facilities=[]\n",
    "hurl=[]\n",
    "\n",
    "while(True):   \n",
    "    nam=driver.find_elements_by_xpath('//h2[@class=\"title title-6\"]')                # Extracting the Desired Element's by xpath\n",
    "    dis=driver.find_elements_by_xpath('//div[@class=\"title-row\"]/div/a/span[1]')\n",
    "    rat=driver.find_elements_by_xpath('//div[@class=\"rating rating-summary-container big\"]')\n",
    "    rev=driver.find_elements_by_xpath('//div[@class=\"rating rating-summary-container big\"]')\n",
    "    tre=driver.find_elements_by_xpath('//div[@class=\"rating rating-summary-container big\"]')\n",
    "    pri=driver.find_elements_by_xpath('//div[@class=\"price-col\"]')\n",
    "    dor=driver.find_elements_by_xpath('//div[@class=\"price-col\"]')\n",
    "    faci=driver.find_elements_by_xpath('//div[@class=\"facilities-label facilities\"]')\n",
    "    urls=driver.find_elements_by_xpath(\"//h2[@class='title title-6']/a\")\n",
    "\n",
    "    for i in nam:\n",
    "        name.append(i.text)                    # Appending the Text Data\n",
    "    for j in dis:\n",
    "        distance.append(j.text)\n",
    "    for k in rat:\n",
    "        ratings.append(k.text.split('\\n')[0])\n",
    "    for l in rev:\n",
    "        reviews.append(l.text.split('\\n')[1])\n",
    "    for m in tre:\n",
    "            treview.append(m.text.split()[2])\n",
    "    for o in pri:\n",
    "        private.append(o.text.replace('\\n',''))\n",
    "    for p in dor:\n",
    "        dorms.append(p.text.replace('\\n',''))\n",
    "    for q in faci:\n",
    "        facilities.append(q.text)\n",
    "    for url in urls:\n",
    "        hurl.append(url.get_attribute('href'))\n",
    "        \n",
    "    for n in reviews:                            # As there were some data in 'OverAll Reviews' column some of the data were missing, so replacing those data by \"none\"\n",
    "        if \"Rating\"in n:\n",
    "            reviews[reviews.index(n)] = \"None\"\n",
    "        elif \"Reviews\"in n:\n",
    "            reviews[reviews.index(n)] = \"None\"\n",
    "       \n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath('//div[@class=\"pagination-item pagination-next\"]')  #Clicking on next button\n",
    "\n",
    "        next_button.click()\n",
    "\n",
    "    except:\n",
    "             break\n",
    "print(len(name))                                # Printing the length of the  Extracted Data\n",
    "print(len(distance))\n",
    "print(len(ratings))\n",
    "print(len(reviews))\n",
    "print(len(treview))\n",
    "print(len(private))\n",
    "print(len(dorms))\n",
    "print(len(facilities))\n",
    "print(len(hurl))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "deed25b3",
   "metadata": {},
   "source": [
    "Here \"Privates from Price\" and \"Dorms from Price\" were under the same class so there data were coming together, so just used range function and extracted the desired data that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfc08c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "p=private[0:200:2]\n",
    "pp=dorms[1:200:2]\n",
    "print(len(p))\n",
    "print(len(pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29385bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Description of each hotels by individually going to each url of each hosel as we have already extracted the url of each Hostel's\n",
    "\n",
    "description = []\n",
    "for i in hurl:    \n",
    "    try:\n",
    "        driver.get(i)\n",
    "        try:\n",
    "            see=driver.find_element_by_xpath('//*[@id=\"__layout\"]/div/div[1]/section/div[6]/div/div[2]/div/div/a')\n",
    "            see.click()\n",
    "        except:\n",
    "            pass   \n",
    "        time.sleep(1)\n",
    "        des=driver.find_element_by_xpath('//div[@class=\"description-container\"]')\n",
    "        description.append(des.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        description.append(\"No Description Availabe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d03c99f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfde233c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hostel Name</th>\n",
       "      <th>Distance From City Centre</th>\n",
       "      <th>Total Reviews</th>\n",
       "      <th>Overall Reviews</th>\n",
       "      <th>Privates From Price</th>\n",
       "      <th>Dorms From Price</th>\n",
       "      <th>Facilities</th>\n",
       "      <th>Property Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mornington Camden</td>\n",
       "      <td>Hostel - 4.1km from city centre</td>\n",
       "      <td>44</td>\n",
       "      <td>Fabulous</td>\n",
       "      <td>No Privates Available</td>\n",
       "      <td>Dorms FromRs3277</td>\n",
       "      <td>Free WiFi</td>\n",
       "      <td>Property Description\\nMornington Camden Hostel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smart Russell Square Hostel</td>\n",
       "      <td>Hostel - 2.6km from city centre</td>\n",
       "      <td>9567</td>\n",
       "      <td>Good</td>\n",
       "      <td>No Privates Available</td>\n",
       "      <td>Dorms FromRs1214</td>\n",
       "      <td>Free WiFi\\nFollows Covid-19 sanitation guidance</td>\n",
       "      <td>Hostelworld says\\nMany Hostelworld guests who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smart Camden Inn Hostel</td>\n",
       "      <td>Hostel - 4.4km from city centre</td>\n",
       "      <td>2740</td>\n",
       "      <td>Fabulous</td>\n",
       "      <td>Privates FromRs5470</td>\n",
       "      <td>Dorms FromRs1733</td>\n",
       "      <td>Free WiFi\\nFollows Covid-19 sanitation guidance</td>\n",
       "      <td>Property Description\\nLocated in the middle of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Selina Camden</td>\n",
       "      <td>Hostel - 5.5km from city centre</td>\n",
       "      <td>15</td>\n",
       "      <td>Superb</td>\n",
       "      <td>Privates FromRs12901</td>\n",
       "      <td>Dorms FromRs4173.85 Rs3548</td>\n",
       "      <td>Free WiFi</td>\n",
       "      <td>Property Description\\nAmong underground music ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Queen Elizabeth Chelsea</td>\n",
       "      <td>Hostel - 5.7km from city centre</td>\n",
       "      <td>Good</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>Privates FromRs4112</td>\n",
       "      <td>Dorms FromRs1821</td>\n",
       "      <td>Free WiFi</td>\n",
       "      <td>Property Description\\nPLEASE NOTE WE ONLY ACCE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Green Rooms</td>\n",
       "      <td>Hotel - 10.8km from city centre</td>\n",
       "      <td>76</td>\n",
       "      <td>Superb</td>\n",
       "      <td>Privates FromRs7844</td>\n",
       "      <td>Dorms FromRs2477</td>\n",
       "      <td>Free WiFi</td>\n",
       "      <td>No Description Availabe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>23 Matheson Road</td>\n",
       "      <td>Bed and Breakfast - 6.2km from city centre</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>Privates FromRs5025</td>\n",
       "      <td>No Dorms Available</td>\n",
       "      <td></td>\n",
       "      <td>No Description Availabe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Park Hotel Essex</td>\n",
       "      <td>Hotel - 24.1km from city centre</td>\n",
       "      <td>109</td>\n",
       "      <td>Fabulous</td>\n",
       "      <td>Privates FromRs11636</td>\n",
       "      <td>No Dorms Available</td>\n",
       "      <td>Free Breakfast\\nFollows Covid-19 sanitation gu...</td>\n",
       "      <td>No Description Availabe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Cranbrook Hotel</td>\n",
       "      <td>Hotel - 14.8km from city centre</td>\n",
       "      <td>58</td>\n",
       "      <td>None</td>\n",
       "      <td>Privates FromRs4148</td>\n",
       "      <td>No Dorms Available</td>\n",
       "      <td>Free Breakfast\\nFollows Covid-19 sanitation gu...</td>\n",
       "      <td>No Description Availabe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>TLK Apartments &amp; Hotel</td>\n",
       "      <td>Hotel - 19.9km from city centre</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Privates FromRs6745</td>\n",
       "      <td>No Dorms Available</td>\n",
       "      <td>Free WiFi</td>\n",
       "      <td>No Description Availabe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Hostel Name                   Distance From City Centre  \\\n",
       "0             Mornington Camden             Hostel - 4.1km from city centre   \n",
       "1   Smart Russell Square Hostel             Hostel - 2.6km from city centre   \n",
       "2       Smart Camden Inn Hostel             Hostel - 4.4km from city centre   \n",
       "3                 Selina Camden             Hostel - 5.5km from city centre   \n",
       "4       Queen Elizabeth Chelsea             Hostel - 5.7km from city centre   \n",
       "..                          ...                                         ...   \n",
       "72                  Green Rooms             Hotel - 10.8km from city centre   \n",
       "73             23 Matheson Road  Bed and Breakfast - 6.2km from city centre   \n",
       "74             Park Hotel Essex             Hotel - 24.1km from city centre   \n",
       "75              Cranbrook Hotel             Hotel - 14.8km from city centre   \n",
       "76       TLK Apartments & Hotel             Hotel - 19.9km from city centre   \n",
       "\n",
       "   Total Reviews Overall Reviews    Privates From Price  \\\n",
       "0             44        Fabulous  No Privates Available   \n",
       "1           9567            Good  No Privates Available   \n",
       "2           2740        Fabulous    Privates FromRs5470   \n",
       "3             15          Superb   Privates FromRs12901   \n",
       "4           Good       Very Good    Privates FromRs4112   \n",
       "..           ...             ...                    ...   \n",
       "72            76          Superb    Privates FromRs7844   \n",
       "73             6            None    Privates FromRs5025   \n",
       "74           109        Fabulous   Privates FromRs11636   \n",
       "75            58            None    Privates FromRs4148   \n",
       "76             0            None    Privates FromRs6745   \n",
       "\n",
       "              Dorms From Price  \\\n",
       "0             Dorms FromRs3277   \n",
       "1             Dorms FromRs1214   \n",
       "2             Dorms FromRs1733   \n",
       "3   Dorms FromRs4173.85 Rs3548   \n",
       "4             Dorms FromRs1821   \n",
       "..                         ...   \n",
       "72            Dorms FromRs2477   \n",
       "73          No Dorms Available   \n",
       "74          No Dorms Available   \n",
       "75          No Dorms Available   \n",
       "76          No Dorms Available   \n",
       "\n",
       "                                           Facilities  \\\n",
       "0                                           Free WiFi   \n",
       "1     Free WiFi\\nFollows Covid-19 sanitation guidance   \n",
       "2     Free WiFi\\nFollows Covid-19 sanitation guidance   \n",
       "3                                           Free WiFi   \n",
       "4                                           Free WiFi   \n",
       "..                                                ...   \n",
       "72                                          Free WiFi   \n",
       "73                                                      \n",
       "74  Free Breakfast\\nFollows Covid-19 sanitation gu...   \n",
       "75  Free Breakfast\\nFollows Covid-19 sanitation gu...   \n",
       "76                                          Free WiFi   \n",
       "\n",
       "                                 Property Description  \n",
       "0   Property Description\\nMornington Camden Hostel...  \n",
       "1   Hostelworld says\\nMany Hostelworld guests who ...  \n",
       "2   Property Description\\nLocated in the middle of...  \n",
       "3   Property Description\\nAmong underground music ...  \n",
       "4   Property Description\\nPLEASE NOTE WE ONLY ACCE...  \n",
       "..                                                ...  \n",
       "72                            No Description Availabe  \n",
       "73                            No Description Availabe  \n",
       "74                            No Description Availabe  \n",
       "75                            No Description Availabe  \n",
       "76                            No Description Availabe  \n",
       "\n",
       "[77 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Creating DataFrame:\n",
    "hostel = pd.DataFrame({'Hostel Name':name,'Distance From City Centre':distance,'Total Reviews':treview,'Overall Reviews':reviews,'Privates From Price':p,'Dorms From Price':pp,'Facilities':facilities,'Property Description':description})\n",
    "hostel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb474890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame as csv file\n",
    "hostel. to_csv(\"Hostel_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "526be7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the Driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db46df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
